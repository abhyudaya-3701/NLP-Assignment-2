{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Information\n",
    "\n",
    "### Project Description\n",
    "\n",
    "In this notebook, we are training and testing a **tokenizer** on **5 samples** of scrapped data. The tokenizer is trained to process raw text data, and we will evaluate its performance by testing it on these 5 samples. \n",
    "\n",
    "### Team Involved:\n",
    "- **Training**: The training was done by **Vinayak Rana** and **Kaloori Shiva Prasad**.\n",
    "- **Testing**: The testing was done by **Abhyudaya Nair**.\n",
    "\n",
    "This exercise demonstrates the effectiveness of the tokenizer in handling real-world, unstructured data and how it can be fine-tuned for specific tasks.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:36:07.224925Z",
     "iopub.status.busy": "2024-11-11T16:36:07.224461Z",
     "iopub.status.idle": "2024-11-11T16:36:07.230100Z",
     "shell.execute_reply": "2024-11-11T16:36:07.229329Z",
     "shell.execute_reply.started": "2024-11-11T16:36:07.224887Z"
    },
    "id": "AH75GkdZbvR7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import argparse\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:31:59.321354Z",
     "iopub.status.busy": "2024-11-11T16:31:59.321007Z",
     "iopub.status.idle": "2024-11-11T16:32:07.306147Z",
     "shell.execute_reply": "2024-11-11T16:32:07.305096Z",
     "shell.execute_reply.started": "2024-11-11T16:31:59.321329Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from gdown) (4.66.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from gdown) (3.16.1)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: PySocks, gdown\n",
      "Successfully installed PySocks-1.7.1 gdown-5.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (4.66.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:32:07.307700Z",
     "iopub.status.busy": "2024-11-11T16:32:07.307418Z",
     "iopub.status.idle": "2024-11-11T16:32:07.311141Z",
     "shell.execute_reply": "2024-11-11T16:32:07.310469Z",
     "shell.execute_reply.started": "2024-11-11T16:32:07.307665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import gdown\n",
    "# gdown.download_folder('https://drive.google.com/drive/folders/1BvQwl3_9fUOTHr5oqN3XidU5CNIgPKqs?usp=sharing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:32:07.312924Z",
     "iopub.status.busy": "2024-11-11T16:32:07.312671Z",
     "iopub.status.idle": "2024-11-11T16:32:07.325478Z",
     "shell.execute_reply": "2024-11-11T16:32:07.324770Z",
     "shell.execute_reply.started": "2024-11-11T16:32:07.312899Z"
    },
    "id": "oAvIF1k3cAtj",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_tokenizer(data_list, vocab_size=32768, model_name=\"tokenizer_sample2\"):\n",
    "\n",
    "    ## Change bos & eos\n",
    "\n",
    "    bos_tok = \"<sos>\"\n",
    "\n",
    "    eos_tok = \"<end_of_sen>\"\n",
    "\n",
    "    ## Add basic characters to this below list, including numbers & special language characters.\n",
    "\n",
    "    special_char = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "\n",
    "    tokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "    tokenizer.train_from_iterator(\n",
    "\n",
    "        data_list,\n",
    "\n",
    "        vocab_size = vocab_size,\n",
    "\n",
    "        min_frequency = 5,\n",
    "\n",
    "        special_tokens = [\"<pad>\", \"<unk>\", bos_tok, eos_tok, \"<user>\", \"<assistant>\"] + special_char,\n",
    "\n",
    "        show_progress = True,\n",
    "\n",
    "    )\n",
    "\n",
    "    ## Don't forget to add special tokens.\n",
    "\n",
    "    transformer_tokenizer = PreTrainedTokenizerFast(\n",
    "\n",
    "        tokenizer_object=tokenizer,\n",
    "\n",
    "        bos_token = bos_tok,\n",
    "\n",
    "        eos_token = eos_tok,\n",
    "\n",
    "        unk_token = \"<unk>\",\n",
    "\n",
    "        pad_token = \"<pad>\",\n",
    "\n",
    "        mask_token = \"<mask>\",\n",
    "\n",
    "        padding_side = \"left\",\n",
    "\n",
    "        truncation_side = \"right\",\n",
    "\n",
    "        additional_special_tokens = [\"<user>\", \"<assistant>\"],\n",
    "\n",
    "        clean_up_tokenization_spaces = False,\n",
    "\n",
    "    )\n",
    "\n",
    "    transformer_tokenizer.save_pretrained(f'Drive/MyDrive/NLP LLM/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T16:08:35.993422Z",
     "iopub.status.busy": "2024-11-10T16:08:35.993179Z",
     "iopub.status.idle": "2024-11-10T16:08:36.002682Z",
     "shell.execute_reply": "2024-11-10T16:08:36.002061Z",
     "shell.execute_reply.started": "2024-11-10T16:08:35.993396Z"
    },
    "id": "1od82dL9Kvjo",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# data_list = []\n",
    "\n",
    "# base_dir = 'Pdf text'\n",
    "\n",
    "# for file in os.listdir(base_dir):\n",
    "\n",
    "#     f_path = os.path.join(base_dir,file)\n",
    "\n",
    "#     with open(f_path,'r',encoding='utf8') as f:\n",
    "\n",
    "#         text = f.read()\n",
    "\n",
    "#     data_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T16:08:36.017273Z",
     "iopub.status.busy": "2024-11-10T16:08:36.017040Z",
     "iopub.status.idle": "2024-11-10T16:08:48.702676Z",
     "shell.execute_reply": "2024-11-10T16:08:48.701898Z",
     "shell.execute_reply.started": "2024-11-10T16:08:36.017248Z"
    },
    "id": "7H2Dg9UupUnG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\"\"\"\" We had taken 5 different samples from our collected data of different sizes. \n",
    "After that we made a csv file for those samples and uploaded on google drive, here we are downloading\n",
    "them using gdown and using them to train the tokenizer\"\"\"\n",
    "\n",
    "df_text = pd.read_csv('sample_2.csv')  \n",
    "\n",
    "train_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T10:44:22.891066Z",
     "iopub.status.busy": "2024-11-11T10:44:22.890679Z",
     "iopub.status.idle": "2024-11-11T10:44:37.412690Z",
     "shell.execute_reply": "2024-11-11T10:44:37.411270Z",
     "shell.execute_reply.started": "2024-11-11T10:44:22.891027Z"
    },
    "id": "_mURIpb65WHE",
    "outputId": "428b3dfa-05c3-42e5-81e4-48082221c9e0",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=09913264228e79ee4121e74f1db751965ed254bb33e0fa0588795696acd76b14\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also tried using cleaning methods like removing other language words, but it took a lot of time so we ended up not using it, considering that we only collected the data from english sources and there are very less chances of any word being of other language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5RiKtaY5Fp2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from langdetect import detect\n",
    "\n",
    "# def clean_text(text):\n",
    "\n",
    "#     if not isinstance(text, str):\n",
    "\n",
    "#         return text  # If text is not a string, return it as-is\n",
    "\n",
    "\n",
    "\n",
    "#     # Remove email addresses\n",
    "\n",
    "#     text = re.sub(r'\\S+@\\S+\\.\\S+', ' ', text)\n",
    "\n",
    "\n",
    "\n",
    "#     # Remove URLs\n",
    "\n",
    "#     text = re.sub(r'http\\S+|www\\S+', ' ', text)\n",
    "\n",
    "#     # Remove numbers\n",
    "\n",
    "#     text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "#     # Remove non-ASCII characters except punctuations\n",
    "\n",
    "#     text = re.sub(r'[^\\w\\s.,!?]', ' ', text)\n",
    "\n",
    "\n",
    "\n",
    "#     # Replace multiple spaces with a single space\n",
    "\n",
    "#     text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "\n",
    "#     # Function to check if the word is English\n",
    "\n",
    "#     def is_english(word):\n",
    "\n",
    "#         try:\n",
    "\n",
    "#             # Detect the language of the word\n",
    "\n",
    "#             return detect(word) == 'en'\n",
    "\n",
    "#         except:\n",
    "\n",
    "#             return False  # In case language detection fails\n",
    "\n",
    "\n",
    "#     # Split text into words and filter out non-English words\n",
    "\n",
    "#     words = text.split()\n",
    "\n",
    "#     cleaned_words = [word for word in words if is_english(word)]\n",
    "\n",
    "#     # Rejoin the words back into a single string\n",
    "\n",
    "#     cleaned_text = \" \".join(cleaned_words)\n",
    "\n",
    "#     return cleaned_text.strip()  # Remove leading and trailing spaces\n",
    "\n",
    "\n",
    "# df_text['cleaned_text'] = df_text['Content'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:32:07.326813Z",
     "iopub.status.busy": "2024-11-11T16:32:07.326457Z",
     "iopub.status.idle": "2024-11-11T16:32:07.337356Z",
     "shell.execute_reply": "2024-11-11T16:32:07.336747Z",
     "shell.execute_reply.started": "2024-11-11T16:32:07.326768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+\\.\\S+', '', text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove emojis\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "    # Remove extra spaces, tabs, and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_text['cleaned_text'] = df_text['Content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "execution": {
     "iopub.status.busy": "2024-11-11T10:44:37.683448Z",
     "iopub.status.idle": "2024-11-11T10:44:37.684047Z",
     "shell.execute_reply": "2024-11-11T10:44:37.683760Z",
     "shell.execute_reply.started": "2024-11-11T10:44:37.683728Z"
    },
    "id": "yGyOaekg5e-z",
    "outputId": "bcf6c5df-00f5-4b39-c775-498f0fc157ac",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_text_cleaned = df_text_cleaned.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T16:10:28.202284Z",
     "iopub.status.busy": "2024-11-10T16:10:28.201964Z",
     "iopub.status.idle": "2024-11-10T16:12:29.709498Z",
     "shell.execute_reply": "2024-11-10T16:12:29.708711Z",
     "shell.execute_reply.started": "2024-11-10T16:10:28.202256Z"
    },
    "id": "al_KywNK5kUb",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_tokenizer(df_text_cleaned['cleaned_text'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:32:07.338469Z",
     "iopub.status.busy": "2024-11-11T16:32:07.338234Z",
     "iopub.status.idle": "2024-11-11T16:32:15.244775Z",
     "shell.execute_reply": "2024-11-11T16:32:15.243998Z",
     "shell.execute_reply.started": "2024-11-11T16:32:07.338443Z"
    },
    "id": "MYzMUs8X0JbN",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:202: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Testing Training Tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:32:15.246448Z",
     "iopub.status.busy": "2024-11-11T16:32:15.245825Z",
     "iopub.status.idle": "2024-11-11T16:32:20.976925Z",
     "shell.execute_reply": "2024-11-11T16:32:20.976196Z",
     "shell.execute_reply.started": "2024-11-11T16:32:15.246415Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1LOOSEwIUedmAj25kGsJAIntQ81haWvaH\n",
      "From (redirected): https://drive.google.com/uc?id=1LOOSEwIUedmAj25kGsJAIntQ81haWvaH&confirm=t&uuid=1427c0b3-8ee8-4ace-a060-4bf643103d3a\n",
      "To: /kaggle/working/propublicadotorg.csv\n",
      "100%|██████████| 197M/197M [00:01<00:00, 124MB/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'propublicadotorg.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdown.download('https://drive.google.com/uc?id=1LOOSEwIUedmAj25kGsJAIntQ81haWvaH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:32:20.978780Z",
     "iopub.status.busy": "2024-11-11T16:32:20.978530Z",
     "iopub.status.idle": "2024-11-11T16:32:23.184922Z",
     "shell.execute_reply": "2024-11-11T16:32:23.184083Z",
     "shell.execute_reply.started": "2024-11-11T16:32:20.978756Z"
    },
    "id": "ttMx6uonZjbY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('propublicadotorg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:32:23.186232Z",
     "iopub.status.busy": "2024-11-11T16:32:23.185931Z",
     "iopub.status.idle": "2024-11-11T16:32:23.190014Z",
     "shell.execute_reply": "2024-11-11T16:32:23.189369Z",
     "shell.execute_reply.started": "2024-11-11T16:32:23.186203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df.columns=['filename','content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:32:23.191137Z",
     "iopub.status.busy": "2024-11-11T16:32:23.190879Z",
     "iopub.status.idle": "2024-11-11T16:32:54.896080Z",
     "shell.execute_reply": "2024-11-11T16:32:54.895265Z",
     "shell.execute_reply.started": "2024-11-11T16:32:23.191112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df['content'] = test_df['content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:32:54.897999Z",
     "iopub.status.busy": "2024-11-11T16:32:54.897708Z",
     "iopub.status.idle": "2024-11-11T16:33:53.454782Z",
     "shell.execute_reply": "2024-11-11T16:33:53.453852Z",
     "shell.execute_reply.started": "2024-11-11T16:32:54.897973Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1-QsNQfVWC2qqNErfn7jetYwUd--A4_rp special_tokens_map.json\n",
      "Processing file 1-YH-b28bEq_-Wl7dvvJr96DMlGXW0A9P tokenizer_config.json\n",
      "Processing file 1-PVwVpAmP3SlTl0ub_s5Lap2ZdCBz-D3 tokenizer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-QsNQfVWC2qqNErfn7jetYwUd--A4_rp\n",
      "To: /kaggle/working/tokenizer_sample1/special_tokens_map.json\n",
      "100%|██████████| 202/202 [00:00<00:00, 895kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-YH-b28bEq_-Wl7dvvJr96DMlGXW0A9P\n",
      "To: /kaggle/working/tokenizer_sample1/tokenizer_config.json\n",
      "100%|██████████| 3.26k/3.26k [00:00<00:00, 6.81MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-PVwVpAmP3SlTl0ub_s5Lap2ZdCBz-D3\n",
      "To: /kaggle/working/tokenizer_sample1/tokenizer.json\n",
      "100%|██████████| 1.42M/1.42M [00:00<00:00, 135MB/s]\n",
      "Download completed\n",
      "Retrieving folder contents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1yGsDkm9p5W9gRBbJCK10TNDtlSYqm-fP special_tokens_map.json\n",
      "Processing file 1ae5KiEkXChWdEjusKd-hH0_oAKtNJOYl tokenizer_config.json\n",
      "Processing file 1FPARh0-vvnMLPmPTh82bjYCT2_zFSvzY tokenizer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1yGsDkm9p5W9gRBbJCK10TNDtlSYqm-fP\n",
      "To: /kaggle/working/tokenizer_sample2/special_tokens_map.json\n",
      "100%|██████████| 202/202 [00:00<00:00, 625kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ae5KiEkXChWdEjusKd-hH0_oAKtNJOYl\n",
      "To: /kaggle/working/tokenizer_sample2/tokenizer_config.json\n",
      "100%|██████████| 3.26k/3.26k [00:00<00:00, 6.44MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1FPARh0-vvnMLPmPTh82bjYCT2_zFSvzY\n",
      "To: /kaggle/working/tokenizer_sample2/tokenizer.json\n",
      "100%|██████████| 2.38M/2.38M [00:00<00:00, 193MB/s]\n",
      "Download completed\n",
      "Retrieving folder contents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1-B_4pu0xv5NdetiWVlcSGNgLkNmobWml special_tokens_map.json\n",
      "Processing file 1-Byt5cNtUDuneHYu01xGFG4Nunw3utF- tokenizer_config.json\n",
      "Processing file 1-929m6VH1INUNk8FCq6dlKfJ3CdQT5PS tokenizer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-B_4pu0xv5NdetiWVlcSGNgLkNmobWml\n",
      "To: /kaggle/working/tokenizer_sample/special_tokens_map.json\n",
      "100%|██████████| 202/202 [00:00<00:00, 709kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-Byt5cNtUDuneHYu01xGFG4Nunw3utF-\n",
      "To: /kaggle/working/tokenizer_sample/tokenizer_config.json\n",
      "100%|██████████| 3.26k/3.26k [00:00<00:00, 6.52MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-929m6VH1INUNk8FCq6dlKfJ3CdQT5PS\n",
      "To: /kaggle/working/tokenizer_sample/tokenizer.json\n",
      "100%|██████████| 1.48M/1.48M [00:00<00:00, 142MB/s]\n",
      "Download completed\n",
      "Retrieving folder contents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1-6u1fvIN01CxwiNpSInuQd3EbMe76J30 special_tokens_map.json\n",
      "Processing file 1-75holrFY-RwhOkjR6qM0tuoO5Q0Yce2 tokenizer_config.json\n",
      "Processing file 1-44ejtkVRIdDG1FNZTNqaBSKTllGbmD1 tokenizer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-6u1fvIN01CxwiNpSInuQd3EbMe76J30\n",
      "To: /kaggle/working/tokenizer_sample5/special_tokens_map.json\n",
      "100%|██████████| 195/195 [00:00<00:00, 876kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-75holrFY-RwhOkjR6qM0tuoO5Q0Yce2\n",
      "To: /kaggle/working/tokenizer_sample5/tokenizer_config.json\n",
      "100%|██████████| 3.24k/3.24k [00:00<00:00, 8.06MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-44ejtkVRIdDG1FNZTNqaBSKTllGbmD1\n",
      "To: /kaggle/working/tokenizer_sample5/tokenizer.json\n",
      "100%|██████████| 1.43M/1.43M [00:00<00:00, 142MB/s]\n",
      "Download completed\n",
      "Retrieving folder contents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1-O3MdvMI5rfkP3blpEp22-3m0DNTcY8Y special_tokens_map.json\n",
      "Processing file 1-R4w_CGBTxJf38gOnPpSvh2VTD1nGkXF tokenizer_config.json\n",
      "Processing file 1-K75ZAV-7G4Gs6JV75FlF1zZWWyFLMY9 tokenizer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-O3MdvMI5rfkP3blpEp22-3m0DNTcY8Y\n",
      "To: /kaggle/working/tokenizer_sample4/special_tokens_map.json\n",
      "100%|██████████| 195/195 [00:00<00:00, 953kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-R4w_CGBTxJf38gOnPpSvh2VTD1nGkXF\n",
      "To: /kaggle/working/tokenizer_sample4/tokenizer_config.json\n",
      "100%|██████████| 3.24k/3.24k [00:00<00:00, 4.91MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-K75ZAV-7G4Gs6JV75FlF1zZWWyFLMY9\n",
      "To: /kaggle/working/tokenizer_sample4/tokenizer.json\n",
      "100%|██████████| 1.43M/1.43M [00:00<00:00, 135MB/s]\n",
      "Download completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/kaggle/working/tokenizer_sample4/special_tokens_map.json',\n",
       " '/kaggle/working/tokenizer_sample4/tokenizer_config.json',\n",
       " '/kaggle/working/tokenizer_sample4/tokenizer.json']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading all the tokenizers from drive to test them\n",
    "\n",
    "url1 = \"https://drive.google.com/drive/folders/1-Ii5eFhtB1TxT28_apSViIz4UcpJ20sT?usp=share_link\"\n",
    "url2 = 'https://drive.google.com/drive/folders/1Un-XZ6vUY85P7N8ZdP3m6wellj8DvSpX?usp=drive_link'\n",
    "url3 = 'https://drive.google.com/drive/folders/1CxNtWVa_kbwVFfrPlG8gPA7cbhxSkBB_?usp=drive_link'\n",
    "url4 = 'https://drive.google.com/drive/folders/1-2ObI4bRChu5ZVn3Fh_yKNM2a-QVFzV7?usp=drive_link'\n",
    "url5 = 'https://drive.google.com/drive/folders/1-EOo1G4hZ1enpaVP2vIviwV5x_GhzCl5?usp=drive_link'\n",
    "gdown.download_folder(url1, quiet=False)\n",
    "gdown.download_folder(url2, quiet=False)\n",
    "gdown.download_folder(url3, quiet=False)\n",
    "gdown.download_folder(url4, quiet=False)\n",
    "gdown.download_folder(url5, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:33:53.456145Z",
     "iopub.status.busy": "2024-11-11T16:33:53.455873Z",
     "iopub.status.idle": "2024-11-11T16:33:53.764297Z",
     "shell.execute_reply": "2024-11-11T16:33:53.763257Z",
     "shell.execute_reply.started": "2024-11-11T16:33:53.456119Z"
    },
    "id": "ewmMgPOS5Hoy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer1 = AutoTokenizer.from_pretrained(\"/kaggle/working/tokenizer_sample1\")\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"/kaggle/working/tokenizer_sample2\")\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(\"/kaggle/working/tokenizer_sample\")\n",
    "tokenizer4 = AutoTokenizer.from_pretrained(\"/kaggle/working/tokenizer_sample4\")\n",
    "tokenizer5 = AutoTokenizer.from_pretrained(\"/kaggle/working/tokenizer_sample5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T16:04:38.687282Z",
     "iopub.status.busy": "2024-11-11T16:04:38.686788Z",
     "iopub.status.idle": "2024-11-11T16:04:38.720925Z",
     "shell.execute_reply": "2024-11-11T16:04:38.719679Z",
     "shell.execute_reply.started": "2024-11-11T16:04:38.687237Z"
    },
    "id": "f01XXs5qAtbx",
    "outputId": "c0b9f8f0-c168-4af8-b9e5-978a37c57aa4",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32769"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:34:01.474876Z",
     "iopub.status.busy": "2024-11-11T16:34:01.474452Z",
     "iopub.status.idle": "2024-11-11T16:34:01.479167Z",
     "shell.execute_reply": "2024-11-11T16:34:01.478407Z",
     "shell.execute_reply.started": "2024-11-11T16:34:01.474844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "matrix = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:36:14.158116Z",
     "iopub.status.busy": "2024-11-11T16:36:14.157686Z",
     "iopub.status.idle": "2024-11-11T16:43:08.521331Z",
     "shell.execute_reply": "2024-11-11T16:43:08.520032Z",
     "shell.execute_reply.started": "2024-11-11T16:36:14.158082Z"
    },
    "id": "6JfXzyIWqyDJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Testing all tokenizers on a sample dataset of 200MB\n",
    "\n",
    "for sample_no in ['sample1','sample2','sample','sample4','sample5']:\n",
    "    f_score = []\n",
    "\n",
    "    if sample_no=='sample1':\n",
    "        tokenizer = tokenizer1\n",
    "    if sample_no=='sample2':\n",
    "        tokenizer = tokenizer2\n",
    "    if sample_no=='sample':\n",
    "        tokenizer = tokenizer3\n",
    "    if sample_no=='sample4':\n",
    "        tokenizer = tokenizer4\n",
    "    if sample_no=='sample5':\n",
    "        tokenizer = tokenizer5\n",
    "        \n",
    "    for text in test_df['content'].to_list():\n",
    "    \n",
    "      input_ids = tokenizer.encode(text)\n",
    "    \n",
    "      input_text = text.split(\" \")\n",
    "    \n",
    "      f_score.append(len(input_ids)/len(input_text))\n",
    "        \n",
    "    matrix[f'tokenizer_{sample_no}'] = np.mean(np.array(f_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T16:58:16.084333Z",
     "iopub.status.busy": "2024-11-11T16:58:16.083417Z",
     "iopub.status.idle": "2024-11-11T16:58:16.089451Z",
     "shell.execute_reply": "2024-11-11T16:58:16.088729Z",
     "shell.execute_reply.started": "2024-11-11T16:58:16.084297Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenizer_sample1': 1.2156464655540813,\n",
       " 'tokenizer_sample2': 1.1974310278241673,\n",
       " 'tokenizer_sample': 1.184354252015691,\n",
       " 'tokenizer_sample4': 1.1847581147241528,\n",
       " 'tokenizer_sample5': 1.3070056610796612}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T17:04:42.518654Z",
     "iopub.status.busy": "2024-11-11T17:04:42.517725Z",
     "iopub.status.idle": "2024-11-11T17:04:42.523281Z",
     "shell.execute_reply": "2024-11-11T17:04:42.522420Z",
     "shell.execute_reply.started": "2024-11-11T17:04:42.518616Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(matrix, orient='index', columns=['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T17:09:15.874138Z",
     "iopub.status.busy": "2024-11-11T17:09:15.873184Z",
     "iopub.status.idle": "2024-11-11T17:09:15.879378Z",
     "shell.execute_reply": "2024-11-11T17:09:15.878577Z",
     "shell.execute_reply.started": "2024-11-11T17:09:15.874097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['Dataset size(MB)'] = [560,450,950,550,560]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERFORMANCE OF ALL TRAINED TOKENIZERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T17:09:21.437733Z",
     "iopub.status.busy": "2024-11-11T17:09:21.437353Z",
     "iopub.status.idle": "2024-11-11T17:09:21.447341Z",
     "shell.execute_reply": "2024-11-11T17:09:21.446543Z",
     "shell.execute_reply.started": "2024-11-11T17:09:21.437703Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>Dataset size(MB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokenizer_sample1</th>\n",
       "      <td>1.215646</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer_sample2</th>\n",
       "      <td>1.197431</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer_sample</th>\n",
       "      <td>1.184354</td>\n",
       "      <td>950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer_sample4</th>\n",
       "      <td>1.184758</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer_sample5</th>\n",
       "      <td>1.307006</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      value  Dataset size(MB)\n",
       "tokenizer_sample1  1.215646               560\n",
       "tokenizer_sample2  1.197431               450\n",
       "tokenizer_sample   1.184354               950\n",
       "tokenizer_sample4  1.184758               550\n",
       "tokenizer_sample5  1.307006               560"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le7lvQQG5HtK",
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30788,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
